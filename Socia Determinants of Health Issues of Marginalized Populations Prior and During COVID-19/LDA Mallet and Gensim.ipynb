{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\scipy\\io\\matlab\\mio5.py:98: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from .mio5_utils import VarReader5\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\_pytest\\mark\\structures.py:443: DeprecationWarning: The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\n",
      "  @attr.s(cmp=False, hash=False)\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "C:\\Users\\clwhitfield\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import os\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_list = ['blacks_pre', 'blacks_post']\n",
    "cities_list = ['nyc','LosAngeles','houston', 'chicago','philadelphia','combined']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_list = ['asians_pre', 'asians_post','blacks_pre', 'blacks_post']\n",
    "cities_list = ['nycThread']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=False))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts,data_words):\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=1, threshold=30) # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values_gensim(dictionary, corpus, texts, limit, start=4, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    # Compute Coherence Score\n",
    "    from gensim.models import CoherenceModel\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        #model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           #iterations = 100,\n",
    "                                           num_topics=num_topics, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values_mallet(mallet_path, dictionary, corpus, texts, limit, start=4, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    from gensim.models import CoherenceModel\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertldaGenToldaMallet(mallet_model):\n",
    "    model_gensim = gensim.models.ldamodel.LdaModel(\n",
    "        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,\n",
    "        alpha=mallet_model.alpha, eta=0,\n",
    "    )\n",
    "    model_gensim.state.sstats[...] = mallet_model.wordtopics\n",
    "    model_gensim.sync_state()\n",
    "    return model_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicModels(sName,c):\n",
    "    output_dir = os.getcwd() + r'/Models/'\n",
    "\n",
    "    df = pd.read_csv(os.getcwd() + r'/data/cleaned_data/%s_%s.csv' %(sName,c), encoding = 'utf-8')\n",
    "    df.cleaned_comments = df.cleaned_comments.apply(str).astype('U').values\n",
    "\n",
    "\n",
    "    df = df[df.cleaned_comments.apply(lambda x: len(x.split(' ')) > 3)]\n",
    "    df = df.reset_index(drop=True)\n",
    "    data = df.cleaned_comments.tolist()\n",
    "    data1 = df.comments.tolist() #to place raw text in sorted topics data\n",
    "    data_words = list(sent_to_words(data))\n",
    "\n",
    "    # Build the bigram and trigram models\n",
    "    \n",
    "    #trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    #trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words,data_words)\n",
    "\n",
    "    # Create Dictionary\n",
    "    id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "    # Create Corpus\n",
    "    texts = data_words_bigrams\n",
    "\n",
    "    # Term Document Frequency\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "    \n",
    "    try: \n",
    "        os.makedirs(output_dir +r'Corpora/', exist_ok = True) \n",
    "        #print(\"Directory '%s' created successfully\" %output_dir) \n",
    "    except OSError as error: \n",
    "            print(\"Directory '%s' can not be created\")\n",
    "    \n",
    "    output_file = '/Corpora/%s_%s_corpus.p'%(sName,c)\n",
    "    with open(r'%s%s'%(output_dir,output_file), 'wb') as fp:\n",
    "        pickle.dump(corpus, fp)\n",
    "\n",
    "    model_list, coherence_values = compute_coherence_values_gensim(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=4, limit=22, step=3)\n",
    "    \n",
    "    limit=22; start=4; step=3;\n",
    "    x = range(start, limit, step)\n",
    "    \n",
    "    # Print the coherence scores  Create write to file\n",
    "    try: \n",
    "        os.makedirs(output_dir +r'Gensim/Text/', exist_ok = True) \n",
    "        #print(\"Directory '%s' created successfully\" %output_dir) \n",
    "    except OSError as error: \n",
    "            print(\"Directory '%s' can not be created\")            \n",
    "    myfile = open(r'%sGensim/Text/%s_%s_coherence.txt'%(output_dir, sName,c),'w+', encoding = 'utf-8')\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        cv = round(cv,4)\n",
    "        myfile.writelines(\"Num Topics = %s has Coherence Value of %s\"%(m, cv))\n",
    "        myfile.writelines('\\n')\n",
    "\n",
    "\n",
    "    # Select the model and print the topics\n",
    "    best_result_index = coherence_values.index(max(coherence_values))\n",
    "    optimal_model = model_list[best_result_index]\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "    #print(optimal_model.print_topics(num_words=10))\n",
    "    myfile.writelines(f'''The {x[best_result_index]} topics gives the highest coherence score \\\n",
    "    of {coherence_values[best_result_index]}''')\n",
    "\n",
    "    myfile.close()\n",
    "    output_file = '/Gensim/%s_%s_model_list.p'%(sName,c)\n",
    "    with open(r'%s%s'%(output_dir,output_file), 'wb') as fp:\n",
    "        pickle.dump(model_list, fp)\n",
    "        \n",
    "    try: \n",
    "        os.makedirs(output_dir +r'Gensim/%s/'%c, exist_ok = True) \n",
    "        #print(\"Directory '%s' created successfully\" %output_dir) \n",
    "    except OSError as error: \n",
    "            print(\"Directory '%s' can not be created\") \n",
    "    # Visualize the topics\n",
    "    \n",
    "\n",
    "    for m in model_list:\n",
    "        pyLDAvis.enable_notebook()\n",
    "        try:\n",
    "            vis = pyLDAvis.gensim_models.prepare(m, corpus, id2word, sort_topics=True)\n",
    "            pyLDAvis.save_html(vis, '%s/Gensim/%s/%s_%s_%s.html'%(output_dir,c,sName,c,model_list.index(m)))\n",
    "        except:\n",
    "                continue\n",
    "    #vis\n",
    "    del model_list\n",
    "\n",
    "    ##BEGIN Mallet\n",
    "\n",
    "    os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'})\n",
    "    mallet_path = r'C:/mallet-2.0.8/bin/mallet.bat' # update this path\n",
    "    #ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "\n",
    "    # Can take a long time to run.\n",
    "    model_list, coherence_values = compute_coherence_values_mallet(mallet_path, dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=4, limit=22, step=3)\n",
    "\n",
    "    limit=22; start=4; step=3;\n",
    "    x = range(start, limit, step)\n",
    "    # Print the coherence scores  Create write to file\n",
    "    try: \n",
    "        os.makedirs(output_dir+r'Mallet/Text/', exist_ok = True) \n",
    "        #print(\"Directory '%s' created successfully\" %output_dir) \n",
    "    except OSError as error: \n",
    "            print(\"Directory '%s' can not be created\")            \n",
    "    myfile = open(r'%sMallet/Text/%s_%s_coherence.txt'%(output_dir, sName,c),'w+', encoding = 'utf-8')\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        cv = round(cv,4)\n",
    "        myfile.writelines(\"Num Topics = %s has Coherence Value of %s\"%(m, cv))\n",
    "        myfile.writelines('\\n')\n",
    "    # Select the model and print the topics\n",
    "    best_result_index = coherence_values.index(max(coherence_values))\n",
    "    optimal_model = model_list[best_result_index]\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "    #print(optimal_model.print_topics(num_words=10))\n",
    "    myfile.writelines(f'''The {x[best_result_index]} topics gives the highest coherence score \\\n",
    "    of {coherence_values[best_result_index]}''')\n",
    "\n",
    "    myfile.close()\n",
    "    output_file = '/Mallet/%s_%s_model_list.p'%(sName,c)\n",
    "    with open(r'%s%s'%(output_dir,output_file), 'wb') as fp:\n",
    "        pickle.dump(model_list, fp)\n",
    "    #optimal_model = convertldaGenToldaMallet(optimal_model) #below - loop through optimal_model create pyLDAviz for each save to file\n",
    "\n",
    "    # Visualize the topics\n",
    "    try: \n",
    "        os.makedirs(output_dir +r'Mallet/%s/'%c, exist_ok = True) \n",
    "        #print(\"Directory '%s' created successfully\" %output_dir) \n",
    "    except OSError as error: \n",
    "            print(\"Directory '%s' can not be created\")\n",
    "            \n",
    "    pyLDAvis.enable_notebook()\n",
    "\n",
    "    for m in model_list:\n",
    "        try:\n",
    "            vis = pyLDAvis.gensim_models.prepare(convertldaGenToldaMallet(m), corpus, id2word, sort_topics=True)\n",
    "            pyLDAvis.save_html(vis, '%s/Mallet/%s/%s_%s_%s.html'%(output_dir,c,sName,c,model_list.index(m)))\n",
    "        except:\n",
    "            continue\n",
    "    #vis\n",
    "    del model_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in eth_list:\n",
    "    for j in cities_list:\n",
    "        getTopicModels(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimal_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-3df608152c4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_topic_sents_keywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_topics_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimal_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf_dominant_topic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_topic_sents_keywords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf_dominant_topic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Document_No'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Dominant_Topic'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Topic_Perc_Contrib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Keywords'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimal_model' is not defined"
     ]
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show ###export df to file\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5515</td>\n",
       "      <td>asian, asians, anti, american, racist, care, t...</td>\n",
       "      <td>fair the labor movement accused asians unscrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>people, asians, asian, virus, americans, china...</td>\n",
       "      <td>speculation that speculation fact fake news kx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>asians, asian, chinese, americans, fuck, point...</td>\n",
       "      <td>ah typical commie tactic deflect misdirect typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5554</td>\n",
       "      <td>schools, school, kids, students, high, test, e...</td>\n",
       "      <td>a progressive assault selective high schools i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>asians, white, blacks, whites, jews, latinos, ...</td>\n",
       "      <td>poc people color check white box asians poc wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic_Num  Topic_Perc_Contrib  \\\n",
       "0        0.0              0.5515   \n",
       "1        1.0              0.6712   \n",
       "2        2.0              0.6579   \n",
       "3        3.0              0.5554   \n",
       "4        4.0              0.2882   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  asian, asians, anti, american, racist, care, t...   \n",
       "1  people, asians, asian, virus, americans, china...   \n",
       "2  asians, asian, chinese, americans, fuck, point...   \n",
       "3  schools, school, kids, students, high, test, e...   \n",
       "4  asians, white, blacks, whites, jews, latinos, ...   \n",
       "\n",
       "                                                Text  \n",
       "0  fair the labor movement accused asians unscrup...  \n",
       "1  speculation that speculation fact fake news kx...  \n",
       "2  ah typical commie tactic deflect misdirect typ...  \n",
       "3  a progressive assault selective high schools i...  \n",
       "4  poc people color check white box asians poc wi...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5515</td>\n",
       "      <td>asian, asians, anti, american, racist, care, t...</td>\n",
       "      <td>fair the labor movement accused asians unscrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>people, asians, asian, virus, americans, china...</td>\n",
       "      <td>speculation that speculation fact fake news kx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>asians, asian, chinese, americans, fuck, point...</td>\n",
       "      <td>ah typical commie tactic deflect misdirect typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5554</td>\n",
       "      <td>schools, school, kids, students, high, test, e...</td>\n",
       "      <td>a progressive assault selective high schools i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>asians, white, blacks, whites, jews, latinos, ...</td>\n",
       "      <td>poc people color check white box asians poc wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6781</td>\n",
       "      <td>racist, people, asians, nyc, blacks, discrimin...</td>\n",
       "      <td>fake news if youre spreading fake news acid at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3894</td>\n",
       "      <td>groups, poor, nyc, minority, poverty, communit...</td>\n",
       "      <td>unfortunately chinese population nyc the poore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4056</td>\n",
       "      <td>black, people, crime, community, violence, asi...</td>\n",
       "      <td>well far fewer asians country black people bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5245</td>\n",
       "      <td>people, post, person, asians, literally, fucki...</td>\n",
       "      <td>like i said i grew policies china i vote i cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>racism, asians, country, group, racial, racist...</td>\n",
       "      <td>i especially asians nyc wearing body cams we n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>city, coronavirus, new_york, covid, year, home...</td>\n",
       "      <td>press release follows dinapoli restaurant indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3532</td>\n",
       "      <td>asians, hate, person, attacked, fight, woman, ...</td>\n",
       "      <td>in beginning february i hookah bar family enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5193</td>\n",
       "      <td>lot, living, work, family, south, east, things...</td>\n",
       "      <td>not info williamsburg yuppies yupsters hipster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5895</td>\n",
       "      <td>asians, chinese, nyc, racist, china, time, tru...</td>\n",
       "      <td>the acid thing talking terrible zero evidence ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  Topic_Perc_Contrib  \\\n",
       "0         0.0              0.5515   \n",
       "1         1.0              0.6712   \n",
       "2         2.0              0.6579   \n",
       "3         3.0              0.5554   \n",
       "4         4.0              0.2882   \n",
       "5         5.0              0.6781   \n",
       "6         6.0              0.3894   \n",
       "7         7.0              0.4056   \n",
       "8         8.0              0.5245   \n",
       "9         9.0              0.7109   \n",
       "10       10.0              0.7378   \n",
       "11       11.0              0.3532   \n",
       "12       12.0              0.5193   \n",
       "13       13.0              0.5895   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   asian, asians, anti, american, racist, care, t...   \n",
       "1   people, asians, asian, virus, americans, china...   \n",
       "2   asians, asian, chinese, americans, fuck, point...   \n",
       "3   schools, school, kids, students, high, test, e...   \n",
       "4   asians, white, blacks, whites, jews, latinos, ...   \n",
       "5   racist, people, asians, nyc, blacks, discrimin...   \n",
       "6   groups, poor, nyc, minority, poverty, communit...   \n",
       "7   black, people, crime, community, violence, asi...   \n",
       "8   people, post, person, asians, literally, fucki...   \n",
       "9   racism, asians, country, group, racial, racist...   \n",
       "10  city, coronavirus, new_york, covid, year, home...   \n",
       "11  asians, hate, person, attacked, fight, woman, ...   \n",
       "12  lot, living, work, family, south, east, things...   \n",
       "13  asians, chinese, nyc, racist, china, time, tru...   \n",
       "\n",
       "                                                 Text  \n",
       "0   fair the labor movement accused asians unscrup...  \n",
       "1   speculation that speculation fact fake news kx...  \n",
       "2   ah typical commie tactic deflect misdirect typ...  \n",
       "3   a progressive assault selective high schools i...  \n",
       "4   poc people color check white box asians poc wi...  \n",
       "5   fake news if youre spreading fake news acid at...  \n",
       "6   unfortunately chinese population nyc the poore...  \n",
       "7   well far fewer asians country black people bla...  \n",
       "8   like i said i grew policies china i vote i cit...  \n",
       "9   i especially asians nyc wearing body cams we n...  \n",
       "10  press release follows dinapoli restaurant indu...  \n",
       "11  in beginning february i hookah bar family enco...  \n",
       "12  not info williamsburg yuppies yupsters hipster...  \n",
       "13  the acid thing talking terrible zero evidence ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Keywords</th>\n",
       "      <th>Num_Documents</th>\n",
       "      <th>Perc_Documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>schools, school, kids, students, education, hi...</td>\n",
       "      <td>296.0</td>\n",
       "      <td>0.0678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>13.0</td>\n",
       "      <td>asian, person, hate, hate_crimes, woman, attac...</td>\n",
       "      <td>188.0</td>\n",
       "      <td>0.0431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>gt, year, times, home, face, new_york, free, t...</td>\n",
       "      <td>314.0</td>\n",
       "      <td>0.0720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>live, lot, south, time, pretty, friends, area,...</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>15.0</td>\n",
       "      <td>asians, racist, talking, race, understand, cou...</td>\n",
       "      <td>239.0</td>\n",
       "      <td>0.0548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4359.0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>asians, chinese, nyc, african, minorities, tim...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4360.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>asians, people, shit, fucking, yeah, literally...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4361.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>virus, china, people, chinese, covid, coronavi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4362.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>live, lot, south, time, pretty, friends, area,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4363.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>virus, china, people, chinese, covid, coronavi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4364 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant_Topic                                     Topic_Keywords  \\\n",
       "0.0                7.0  schools, school, kids, students, education, hi...   \n",
       "1.0               13.0  asian, person, hate, hate_crimes, woman, attac...   \n",
       "2.0               18.0  gt, year, times, home, face, new_york, free, t...   \n",
       "3.0                0.0  live, lot, south, time, pretty, friends, area,...   \n",
       "4.0               15.0  asians, racist, talking, race, understand, cou...   \n",
       "...                ...                                                ...   \n",
       "4359.0             8.0  asians, chinese, nyc, african, minorities, tim...   \n",
       "4360.0             2.0  asians, people, shit, fucking, yeah, literally...   \n",
       "4361.0            10.0  virus, china, people, chinese, covid, coronavi...   \n",
       "4362.0             0.0  live, lot, south, time, pretty, friends, area,...   \n",
       "4363.0            10.0  virus, china, people, chinese, covid, coronavi...   \n",
       "\n",
       "        Num_Documents  Perc_Documents  \n",
       "0.0             296.0          0.0678  \n",
       "1.0             188.0          0.0431  \n",
       "2.0             314.0          0.0720  \n",
       "3.0             224.0          0.0513  \n",
       "4.0             239.0          0.0548  \n",
       "...               ...             ...  \n",
       "4359.0            NaN             NaN  \n",
       "4360.0            NaN             NaN  \n",
       "4361.0            NaN             NaN  \n",
       "4362.0            NaN             NaN  \n",
       "4363.0            NaN             NaN  \n",
       "\n",
       "[4364 rows x 4 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of Documents for Each Topic\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Documents for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show ###create condition to drop rows if Num_Documents = NaN. Export to file\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics.to_csv(os.getcwd() + r'\\test.csv', index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic_Num</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5515</td>\n",
       "      <td>asian, asians, anti, american, racist, care, t...</td>\n",
       "      <td>fair the labor movement accused asians unscrup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6712</td>\n",
       "      <td>people, asians, asian, virus, americans, china...</td>\n",
       "      <td>speculation that speculation fact fake news kx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.6579</td>\n",
       "      <td>asians, asian, chinese, americans, fuck, point...</td>\n",
       "      <td>ah typical commie tactic deflect misdirect typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5554</td>\n",
       "      <td>schools, school, kids, students, high, test, e...</td>\n",
       "      <td>a progressive assault selective high schools i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.2882</td>\n",
       "      <td>asians, white, blacks, whites, jews, latinos, ...</td>\n",
       "      <td>poc people color check white box asians poc wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.6781</td>\n",
       "      <td>racist, people, asians, nyc, blacks, discrimin...</td>\n",
       "      <td>fake news if youre spreading fake news acid at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.3894</td>\n",
       "      <td>groups, poor, nyc, minority, poverty, communit...</td>\n",
       "      <td>unfortunately chinese population nyc the poore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4056</td>\n",
       "      <td>black, people, crime, community, violence, asi...</td>\n",
       "      <td>well far fewer asians country black people bla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>0.5245</td>\n",
       "      <td>people, post, person, asians, literally, fucki...</td>\n",
       "      <td>like i said i grew policies china i vote i cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.7109</td>\n",
       "      <td>racism, asians, country, group, racial, racist...</td>\n",
       "      <td>i especially asians nyc wearing body cams we n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.7378</td>\n",
       "      <td>city, coronavirus, new_york, covid, year, home...</td>\n",
       "      <td>press release follows dinapoli restaurant indu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.3532</td>\n",
       "      <td>asians, hate, person, attacked, fight, woman, ...</td>\n",
       "      <td>in beginning february i hookah bar family enco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.5193</td>\n",
       "      <td>lot, living, work, family, south, east, things...</td>\n",
       "      <td>not info williamsburg yuppies yupsters hipster...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.5895</td>\n",
       "      <td>asians, chinese, nyc, racist, china, time, tru...</td>\n",
       "      <td>the acid thing talking terrible zero evidence ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic_Num  Topic_Perc_Contrib  \\\n",
       "0         0.0              0.5515   \n",
       "1         1.0              0.6712   \n",
       "2         2.0              0.6579   \n",
       "3         3.0              0.5554   \n",
       "4         4.0              0.2882   \n",
       "5         5.0              0.6781   \n",
       "6         6.0              0.3894   \n",
       "7         7.0              0.4056   \n",
       "8         8.0              0.5245   \n",
       "9         9.0              0.7109   \n",
       "10       10.0              0.7378   \n",
       "11       11.0              0.3532   \n",
       "12       12.0              0.5193   \n",
       "13       13.0              0.5895   \n",
       "\n",
       "                                             Keywords  \\\n",
       "0   asian, asians, anti, american, racist, care, t...   \n",
       "1   people, asians, asian, virus, americans, china...   \n",
       "2   asians, asian, chinese, americans, fuck, point...   \n",
       "3   schools, school, kids, students, high, test, e...   \n",
       "4   asians, white, blacks, whites, jews, latinos, ...   \n",
       "5   racist, people, asians, nyc, blacks, discrimin...   \n",
       "6   groups, poor, nyc, minority, poverty, communit...   \n",
       "7   black, people, crime, community, violence, asi...   \n",
       "8   people, post, person, asians, literally, fucki...   \n",
       "9   racism, asians, country, group, racial, racist...   \n",
       "10  city, coronavirus, new_york, covid, year, home...   \n",
       "11  asians, hate, person, attacked, fight, woman, ...   \n",
       "12  lot, living, work, family, south, east, things...   \n",
       "13  asians, chinese, nyc, racist, china, time, tru...   \n",
       "\n",
       "                                                 Text  \n",
       "0   fair the labor movement accused asians unscrup...  \n",
       "1   speculation that speculation fact fake news kx...  \n",
       "2   ah typical commie tactic deflect misdirect typ...  \n",
       "3   a progressive assault selective high schools i...  \n",
       "4   poc people color check white box asians poc wi...  \n",
       "5   fake news if youre spreading fake news acid at...  \n",
       "6   unfortunately chinese population nyc the poore...  \n",
       "7   well far fewer asians country black people bla...  \n",
       "8   like i said i grew policies china i vote i cit...  \n",
       "9   i especially asians nyc wearing body cams we n...  \n",
       "10  press release follows dinapoli restaurant indu...  \n",
       "11  in beginning february i hookah bar family enco...  \n",
       "12  not info williamsburg yuppies yupsters hipster...  \n",
       "13  the acid thing talking terrible zero evidence ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_topics_sorteddf_mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
