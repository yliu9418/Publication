{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import os\n",
    "import spacy\n",
    "import urllib.parse as url\n",
    "import re, string, unicodedata\n",
    "from psaw import PushshiftAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()\n",
    "#subred_list = ['nyc','LosAngeles','houston', 'chicago','philadelphia']\n",
    "#keyword_list = ['asians', 'Asian%20American', 'asian%20people','asian%20person', 'zzxibussw']\n",
    "start_epoch=int(dt.datetime(2018, 7, 2).timestamp())\n",
    "#end_epoch = int(dt.datetime(2018, 2, 1).timestamp())\n",
    "end_epoch = int(dt.datetime(2018, 12, 31).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keyword_list = ['asian%20american', 'Asian%20people','Asian%20person']\n",
    "keyword_list = ['asians','blacks']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ascii_to_string(text):\n",
    "    \"\"\"convert ascii text to string\"\"\"\n",
    "    import html\n",
    "    return html.unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_period(text):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"\\w$\", \"\\g<0>.\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(word):\n",
    "    \"\"\"Remove punctuation from sample string\"\"\"\n",
    "    list_of_char = ['>', '*', '\\\"', '#', ]\n",
    "    pattern = '[' + ''.join(list_of_char) + ']'\n",
    "    return re.sub(pattern, '', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize(text):\n",
    "    \"\"\"captilaize first word in sentence\"\"\"\n",
    "    return text.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_leadspace(text):\n",
    "    \"\"\"remove leading whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "    #return re.sub(r'\\t+', \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(text, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi_periods(text):\n",
    "    \"\"\"remove ellipses from text and replace with a single period\"\"\"\n",
    "    return re.sub(r'\\.+', \". \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi_xmarks(text):\n",
    "    \"\"\"remove repeating exlamations from text and replace with a single one\"\"\"\n",
    "    return re.sub(r'\\!+', \"! \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi_qmarks(text):\n",
    "    \"\"\"remove repeating question marks from text and replace with a single one\"\"\"\n",
    "    return re.sub(r'\\?+', \"? \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text,k):\n",
    "    doc = nlp(str(text))\n",
    "    result = ''\n",
    "    for sent in doc.sents:\n",
    "        if url.unquote(k).casefold() in str(sent.text).casefold():\n",
    "            if len(str(sent)) < 25:\n",
    "                continue\n",
    "            if \"bot\".casefold() in str(sent.text).casefold():\n",
    "                continue\n",
    "            if '[' in str(sent.text) or '(' in str(sent.text):\n",
    "                continue\n",
    "            result += capitalize(replace_period(str(sent.text))) + '\\n'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(keyword):\n",
    "    results = api.search_comments( q=keyword, after=start_epoch, before=end_epoch)#,limit = 2000)\n",
    "    cache = []\n",
    "    ID = []\n",
    "    for c in results:\n",
    "        cache.append(c)\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_words = {'asians':'Asians','america':'America','whites':'Whites','arab':'Arab',\n",
    "                'europe':'Europe','hispanic':'Hispanic','latin':'Latin','africa':'Africa',\n",
    "                'mexic':'Mexic','blacks':'Blacks','india':'India','filipin':'Filipin',\n",
    "                'korea':'Korea','china':'China','japan':'Japan','chinese':'Chinese',\n",
    "                'caucasian':'Caucasian','asian American':'Asian American','north America':'North America',\n",
    "                'middle east':'Middle East','white America':'White America','black America':'Black America',\n",
    "                'white pe':'White pe','black pe':'Black pe','pakistan':'Pakistan',\n",
    "                'black wom':'Black wom','white wom':'White wom','black man':'Black man',\n",
    "                'white man':'White man','black men':'Black men','white men':'White men',\n",
    "                'native American':'Native American','brazil':'Brazil','asian man':'Asian man',\n",
    "                'asian wom':'Asian wom','asian pe':'Asian pe',}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveResults(sub_list,keyword):\n",
    "    commentData = []\n",
    "    sentence = []\n",
    "    for comment in sub_list:\n",
    "        commentData.append([comment.body])\n",
    "    df = pd.DataFrame(commentData,columns=['comments'])\n",
    "    df.style.set_properties(subset=[\"comments\"], **{'text-align': 'left'})\n",
    "    df.comments = df.comments.apply(convert_ascii_to_string).astype('U').values\n",
    "    df.comments = df.comments.apply(convert_ascii_to_string).astype('U').values\n",
    "    df.comments = df.comments.apply(str).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_URL).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_punct).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_multi_periods).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_multi_xmarks).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_multi_qmarks).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_non_ascii).astype('U').values\n",
    "    df.comments = df.comments.apply(get_sentences, k = keyword).astype('U').values\n",
    "    df.comments = df.comments.apply(remove_whitespace).astype('U').values\n",
    "    df.comments = df.comments.apply(replace_period).astype('U').values\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    for old, new in replace_words.items():\n",
    "        df.comments = df.comments.str.replace(old, new, regex=False)\n",
    "    #print(df.comments.to_markdown())\n",
    "    with open(os.getcwd() + r'/data/sentence_samples/%s.txt'%keyword, 'a') as f:\n",
    "        np.savetxt(f,df.comments, fmt='%s', newline='')\n",
    "    #with open(os.getcwd() + r'/data/sentence_samples/%s.txt'%keyword,'w+', encoding = 'utf-8') as f:\n",
    "        #print(*df.comments.astype('U').values, file = f )#sep = \"\\n\", file=f)\n",
    "    \n",
    "    \n",
    "    #np.savetxt(os.getcwd() + r'/data/sentence_samples/%s.txt'%keyword, df.comments, fmt='%s', newline='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in keyword_list:\n",
    "         saveResults(getData(i),i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
